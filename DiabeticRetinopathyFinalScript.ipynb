{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiabeticRetinopathyFinalScript.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Essentials "
      ],
      "metadata": {
        "id": "ZDICY1DjVyky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations==0.4.6   # Used For Augmentations \n",
        "!pip install efficientnet_pytorch    # Library for Model Creation\n",
        "!pip install tqdm                    # Interactive Loop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i95uo4AgFRsf",
        "outputId": "ce142ba0-aab4-4e9d-88ea-ee7f7db14100"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.1.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.10.0.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=848c7996f3a4a18e415eb4b21f41220fe5f18ba91f67d011d5ac3d031f68f366\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=619e64b394809bc7a69ff5819003cc3c308b4a8f4ee3e442856f34d5dd17855f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Drive "
      ],
      "metadata": {
        "id": "H8k87YxOV2zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')          # The Dataset is Uploaded on Drive because of its size "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnbGqIlkGQoH",
        "outputId": "9b9dd7af-5eae-461e-e0ec-f53423afb2a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Zip File"
      ],
      "metadata": {
        "id": "eSOPjZSaW0qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/DBTRB/aptos2019-blindness-detection.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"APTOS\")    # extract in APTOS Folder"
      ],
      "metadata": {
        "id": "uS75wZyEFS11"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Essentials"
      ],
      "metadata": {
        "id": "zDca1DlaDuO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wiaY_BWVDsvX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from torch import nn , optim \n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "from torch.utils.data import Dataset , DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ],
      "metadata": {
        "id": "S2qFGrYFEGS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 3e-4\n",
        "WEIGHT_DECAY = 5e-4\n",
        "NUM_EPOCHS = 10\n",
        "NUM_WORKERS = 4\n",
        "CHECKPOINT = 'checkpoint.pth.tar'\n",
        "SAVE_MODEL = True\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "\n",
        "\n",
        "# Training Augmentations or transforms\n",
        "TRAIN_Transforms = A.Compose(\n",
        "    [\n",
        "      A.Resize(width=300,height=300),       \n",
        "      A.RandomCrop(height=256,width=256),   \n",
        "      A.HorizontalFlip(p=0.5),              \n",
        "      A.VerticalFlip(p=0.5),\n",
        "      A.RandomRotate90(p=0.5),\n",
        "      A.Blur(p=0.3),\n",
        "      A.CLAHE(p=0.3),\n",
        "      A.CoarseDropout(max_holes=12, max_height=20, max_width=20, p=0.3),\n",
        "      A.IAAAffine(shear=30, rotate=0, p=0.2, mode=\"constant\"),\n",
        "      ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Validation Augmentations or transforms\n",
        "VAL_Transforms = A.Compose(\n",
        "    [\n",
        "     A.Resize(width=256,height=256),\n",
        "     ToTensorV2()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "RDrtS3o-EIOp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "F_9v2P-_KuWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Accuracy for the Data\n",
        "\n",
        "def get_accuracy(model , loader ):\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  all_labels = []\n",
        "  all_preds = []\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "\n",
        "\n",
        "  for x , y, f in tqdm(loader):\n",
        "\n",
        "    # Making Device gpu if available \n",
        "    x = x.to(DEVICE) \n",
        "    y = y.to(DEVICE)\n",
        "    \n",
        "    x = x.float()\n",
        "\n",
        "    # Computing Scores \n",
        "    with torch.no_grad():\n",
        "      scores = model(x)   # Getting Scores\n",
        "\n",
        "    _ , preds = scores.max(1) # Taking that class with maximum probablistic confidence\n",
        "\n",
        "\n",
        "    num_correct += (preds == y).sum()\n",
        "    num_samples += preds.shape[0]\n",
        "\n",
        "    all_preds.append(preds.detach().cpu().numpy())\n",
        "    all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "  # Getting Accuracy\n",
        "  print(f\"Got an accuracy on {num_correct}/{num_samples} which is {float(num_correct) / float(num_samples)*100:.2f}\")\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  return np.concatenate(all_preds , axis = 0 , dtype = np.int64) , np.concatenate(all_labels, axis = 0 , dtype = np.int64)  \n",
        "\n",
        "# Saving Checkpoint \n",
        "def save_checkpoint(state , filename = \"checkpoint_0.pth.tar\"):\n",
        "  \"<==== SAVING MODEL ====>\"\n",
        "  torch.save(state,f=filename)\n",
        "\n",
        "# Loading Checkpoint\n",
        "def checkpoint_load(model ,checkpoint , optimizer , lr):\n",
        "  \"<==== LOADING MODEL ====>\"\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])"
      ],
      "metadata": {
        "id": "mcyI9n9aKxF4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Custom Dataset Function"
      ],
      "metadata": {
        "id": "t3mmL3bvGFCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MAKEDATASET(Dataset):\n",
        "  def __init__(self , path_image_folder , path_to_csv , train = False , transforms = None):\n",
        "    super().__init__()\n",
        "    self.path_image_folder = path_image_folder\n",
        "    self.path_to_csv = path_to_csv\n",
        "    self.data = pd.read_csv(path_to_csv)\n",
        "    self.train = train \n",
        "    self.transforms = transforms \n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.data.shape[0]\n",
        "  \n",
        "  def __getitem__(self,indx):\n",
        "    img_file , label = self.data.loc[indx]\n",
        "    \n",
        "    image = np.array(Image.open(os.path.join(self.path_image_folder,img_file + \".png\"))) # loading image\n",
        "    \n",
        "    if self.transforms:\n",
        "      image = self.transforms(image=image)[\"image\"]\n",
        "    \n",
        "    return image , label , img_file"
      ],
      "metadata": {
        "id": "dYLS2yEyFaWk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data  = pd.read_csv(\"/content/APTOS/train.csv\")"
      ],
      "metadata": {
        "id": "vkdqN7UheGAx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Class Weights"
      ],
      "metadata": {
        "id": "dza-mthkYgZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.array([0,1,2,3,4]),y=Data['diagnosis'].values)\n",
        "\n",
        "class_weights = torch.tensor(class_weights,dtype=torch.float).to(DEVICE)\n",
        "\n",
        "print(class_weights) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUMJDU-IUCyc",
        "outputId": "cc86ef99-6a46-4f14-b3e4-8b108e12ff3e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4058, 1.9795, 0.7331, 3.7948, 2.4827], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Data in Train and Test set"
      ],
      "metadata": {
        "id": "1Q7p-irqYlwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(Data,\n",
        "                              stratify=Data[['diagnosis']], \n",
        "                              test_size=0.2)\n",
        "\n",
        "train.to_csv(\"Train_Data.csv\",index=False)\n",
        "test.to_csv(\"Val_Data.csv\",index=False)"
      ],
      "metadata": {
        "id": "5skGoYlVUEuy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data"
      ],
      "metadata": {
        "id": "VDCLqsV_YpWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Dataset Object\n",
        "CompleteTrainData = MAKEDATASET(\n",
        "    \"/content/APTOS/train_images\",\n",
        "    \"/content/APTOS/train.csv\",\n",
        "    True,\n",
        "    TRAIN_Transforms)\n",
        "\n",
        "Train_DS = MAKEDATASET(\n",
        "    \"/content/APTOS/train_images\",\n",
        "    \"/content/Train_Data.csv\",\n",
        "    True,\n",
        "    TRAIN_Transforms)\n",
        "\n",
        "Val_DS = MAKEDATASET(\n",
        "    \"/content/APTOS/train_images\",\n",
        "    \"/content/Val_Data.csv\",\n",
        "    True,\n",
        "    VAL_Transforms)\n",
        "\n",
        "# Making Data Loader\n",
        "\n",
        "CompleteTrainData_Loader = DataLoader(\n",
        "    CompleteTrainData,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=True\n",
        "\n",
        ")\n",
        "\n",
        "Train_loader = DataLoader(\n",
        "    Train_DS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "Val_loader = DataLoader(\n",
        "    Val_DS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    num_workers=2,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=False,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLTGgKmzUGwX",
        "outputId": "c333b3f9-f119-4c97-de24-be3cb2627998"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "pxRNU_Qe4JLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Train_One_Epoch(loader , model , optimizer , loss_fn , scaler , device):\n",
        "\n",
        "  Losses  =  []\n",
        "  loop = tqdm(loader) \n",
        "\n",
        "  for batch_idx , (data , target, filename) in enumerate(loop):\n",
        "\n",
        "    # Getting Data to Gpu if available \n",
        "    data = data.to(DEVICE)\n",
        "    target = target.to(DEVICE)\n",
        "\n",
        "    data = data.float()\n",
        "    \n",
        "    # Compute Scores and Loss \n",
        "    with torch.cuda.amp.autocast():\n",
        "      scores = model(data)\n",
        "      loss   = loss_fn(scores , target)\n",
        "\n",
        "    Losses.append(loss.item())\n",
        "\n",
        "    # Perform weight updates \n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "  print(f\"Losses average over epoch : {sum(Losses) / len(Losses)}\")"
      ],
      "metadata": {
        "id": "r3skWWaZIUZh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "2AMrcvjEe9jP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficientnet Model"
      ],
      "metadata": {
        "id": "2_RX0CXWGtNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Using Cross Entropy Loss\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Creating Model Instance \n",
        "model   = EfficientNet.from_pretrained(\"efficientnet-b3\")\n",
        "model._fc = nn.Linear(1536 , 5) # specifying Class\n",
        "model   = model.to(DEVICE)\n",
        "\n",
        "\n",
        "# Using Adam as the optimizer for efficient weight updates\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Scaler is used to performing weight updates\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(6):\n",
        "  Train_One_Epoch(Train_loader , model , optimizer , loss_fn , scaler , DEVICE)\n",
        "\n",
        "  preds , labels = get_accuracy(model, Val_loader)\n",
        "  print(f\"Quadratic Weighted Kappa Score : ( Validation ): {cohen_kappa_score(preds , labels , weights = 'quadratic')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dUIDygcOu7a",
        "outputId": "35c82cb9-8fb6-463a-ffbc-e22864665d8d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/184 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 184/184 [05:15<00:00,  1.71s/it, loss=1.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses average over epoch : 1.1894989311695099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [01:14<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an accuracy on 501/733 which is 68.35\n",
            "Quadratic Weighted Kappa Score : ( Validation ): 0.8102498032859995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/184 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 184/184 [05:12<00:00,  1.70s/it, loss=2.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses average over epoch : 0.9550899847046189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [01:16<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an accuracy on 569/733 which is 77.63\n",
            "Quadratic Weighted Kappa Score : ( Validation ): 0.885753875285516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/184 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 184/184 [05:14<00:00,  1.71s/it, loss=1.49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses average over epoch : 0.8634628282616967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [01:15<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an accuracy on 574/733 which is 78.31\n",
            "Quadratic Weighted Kappa Score : ( Validation ): 0.889426324982302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/184 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 184/184 [05:18<00:00,  1.73s/it, loss=2.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses average over epoch : 0.8567651085879492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [01:16<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an accuracy on 580/733 which is 79.13\n",
            "Quadratic Weighted Kappa Score : ( Validation ): 0.8938789090307139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/184 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 184/184 [05:20<00:00,  1.74s/it, loss=1.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses average over epoch : 0.807983191764873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [01:15<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an accuracy on 589/733 which is 80.35\n",
            "Quadratic Weighted Kappa Score : ( Validation ): 0.8934677236346577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/184 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 184/184 [05:21<00:00,  1.75s/it, loss=0.742]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses average over epoch : 0.7453606329534365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [01:17<00:00,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an accuracy on 565/733 which is 77.08\n",
            "Quadratic Weighted Kappa Score : ( Validation ): 0.8583519322994444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Apply_Thresholding(MyData , Loader,threshold = 0.80):\n",
        "  model.eval()\n",
        "  exps = 0\n",
        "  for indx , (x , y , f) in enumerate(Loader):\n",
        "    x = x.to(DEVICE)\n",
        "    y = y.to(DEVICE)\n",
        "    x = x.float()\n",
        "    with torch.no_grad():\n",
        "      scores = model(x)\n",
        "    sm = torch.nn.Softmax()\n",
        "    probabilities = sm(scores)\n",
        "\n",
        "    top_p, top_class = probabilities.topk(1, dim = 1)\n",
        "    for id , vl in enumerate(top_p):\n",
        "      if vl < threshold:\n",
        "        exps += 1\n",
        "        MyData.drop(MyData.loc[MyData['id_code'] == f[id]].index,inplace=True)\n",
        "  print(f\"The Number of Data Points remove : {exps}\")\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "QIv9R1pvVdGf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data Points before Thresholding {len(Data)}\")\n",
        "Apply_Thresholding(Data, CompleteTrainData_Loader,threshold=0.80)\n",
        "print(f\"Data Points After Thresholding {len(Data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M2AQNGBI725",
        "outputId": "f973d471-4fed-4f8b-c555-2123a15adecc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Points before Thresholding 3662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Number of Data Points remove : 1543\n",
            "Data Points After Thresholding 2119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data.to_csv(\"DataAfterThresholding.csv\",index=False)"
      ],
      "metadata": {
        "id": "_52d0GjUVdAb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_AT = pd.read_csv(\"DataAfterThresholding.csv\")"
      ],
      "metadata": {
        "id": "btCcZIIhnF_s"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.array([0,1,2,3,4]),y=Data_AT['diagnosis'].values)\n",
        "\n",
        "class_weights = torch.tensor(class_weights,dtype=torch.float).to(DEVICE)\n",
        "\n",
        "print(class_weights) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g9UYb4jbh4K",
        "outputId": "0c0ae20d-99c5-4cef-c872-ac09d1532742"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2532, 2.7167, 3.6852, 9.8558, 3.2351], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data_AT = pd.read_csv(\"/content/DataAfterThresholding.csv\")\n",
        "\n",
        "\n",
        "train, test = train_test_split(Data_AT,\n",
        "                              stratify=Data_AT[['diagnosis']], \n",
        "                              test_size=0.2)\n",
        "\n",
        "train.to_csv(\"Train_Data_AT.csv\",index=False)\n",
        "test.to_csv(\"Val_Data_AT.csv\",index=False)"
      ],
      "metadata": {
        "id": "3DpkL4mfVc9s"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_DS = MAKEDATASET(\n",
        "    \"/content/APTOS/train_images\",\n",
        "    \"/content/Train_Data_AT.csv\",\n",
        "    True,\n",
        "    TRAIN_Transforms)\n",
        "\n",
        "Val_DS = MAKEDATASET(\n",
        "    \"/content/APTOS/train_images\",\n",
        "    \"/content/Val_Data_AT.csv\",\n",
        "    True,\n",
        "    VAL_Transforms)\n",
        "\n",
        "Test_DS = MAKEDATASET(\n",
        "    \"/content/APTOS/test_images\",\n",
        "    \"/content/APTOS/test.csv\",\n",
        "    VAL_Transforms)\n",
        "\n",
        "Test_loader  = DataLoader(Test_DS,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS,shuffle=False)\n",
        "\n",
        "CompleteTrainData_Loader = DataLoader(\n",
        "    CompleteTrainData,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=True\n",
        "\n",
        ")\n",
        "\n",
        "Train_loader = DataLoader(\n",
        "    Train_DS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "Val_loader = DataLoader(\n",
        "    Val_DS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    num_workers=2,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BJjElZbVc1E",
        "outputId": "05be8fa8-1a43-491d-f551-5c1122bf9e27"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model   = EfficientNet.from_pretrained(\"efficientnet-b3\")\n",
        "model._fc = nn.Linear(1536 , 5)\n",
        "model   = model.to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(3):\n",
        "  Train_One_Epoch(Train_loader , model , optimizer , loss_fn , scaler , DEVICE)\n",
        "\n",
        "  preds , labels = get_accuracy(model, Val_loader)\n",
        "  print(f\"Quadratic Weighted Kappa Score : ( Validation ): {cohen_kappa_score(preds , labels , weights = 'quadratic')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Qg99RFZLv7",
        "outputId": "432d77db-3d93-459e-f2b2-7baa4d7392dc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/106 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 106/106 [02:25<00:00,  1.37s/it, loss=0.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses average over epoch : 0.4855064534634914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:31<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an accuracy on 364/424 which is 85.85\n",
            "Quadratic Weighted Kappa Score : ( Validation ): 0.7774203636688558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/106 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 106/106 [02:19<00:00,  1.31s/it, loss=0.195]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses average over epoch : 0.2839576209069423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:30<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an accuracy on 398/424 which is 93.87\n",
            "Quadratic Weighted Kappa Score : ( Validation ): 0.9164301482182277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/106 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 106/106 [02:16<00:00,  1.29s/it, loss=0.162]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses average over epoch : 0.25132192425289246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:29<00:00,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got an accuracy on 391/424 which is 92.22\n",
            "Quadratic Weighted Kappa Score : ( Validation ): 0.9330971984098574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Model"
      ],
      "metadata": {
        "id": "83B1C_-TaAKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "    \"state_dict\": model.state_dict(),\n",
        "    \"optimizer\": optimizer.state_dict(),\n",
        "}\n",
        "save_checkpoint(checkpoint, filename=f\"chk.pth.tar\")"
      ],
      "metadata": {
        "id": "HLrHcCwElUXS"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TXZG43YlPkRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pqFGSvavPkOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V8Nl5Pf8PkMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mtdu1ClMPkKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jOGuKBLWiVv9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}